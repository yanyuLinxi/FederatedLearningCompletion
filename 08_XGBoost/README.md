
停止增长的条件：
1. 叶子节点的分裂gain<=0
2. 叶子节点只有一个样本的时候，就没必要划分了。
3. 限制叶子节点个数

计算出叶子节点的b_j。计算完后叶子节点的权重就为b_j

步骤：
1. 

xgboost的训练方式和gbdt类似。计算m-1 f(x)的损失函数在训练样本点的一阶导、二阶导。然后用来分裂每个节点，生成树。最后加到模型中去f_{m-1}(x)+f_m(x)。再计算损失函数，拟合下一个。

# 优化

优化思路：1. 压缩特征数（列采样） 2. 压缩每个特征下，特征的种类数（即特征的分裂点的数量）

1. 预防过拟合
   1. 正则化
   2. 限制节点数量
   3. 限制Gain的增长。当Gain < gamma则不分裂
   4. 列采样。随机采样特征
      1. 按树随机。每一颗树筛选出来特征采样后，一整颗树的特征都不变
      2. 按层随机。每一层叶子节点分裂时，都进行采样。
   5. shrinkage 学习率
      1. 加入学习率到基学习器中。$y=y_{m-1}+\eta f(x)$
      2. 有助于防止过拟合。一般取0.1.
2. 速度优化：
   1. 近似分裂算法：
      1. 使用二阶导代替样本的权重，然后根据权重划分桶。设定一个值$\epsilon$,每个桶的权重值和应该小于$\epsilon$。会得到$\frac{1}{\epsilon}$个切分点。
      2. 为什么使用二阶导？二阶导可以看成是样本的权重。同时二阶导越大，说明该值附近变换越快，越应该进行切分。
      3. 和采样类似。有全局策略和局部策略。
         1. 全局策略。第一次计算得到的分裂点，后面的分裂只会用这几个分裂点。
         2. 局部策略。每次分裂节点时，都会重新计算分裂点。优点是可以根据节点中的样本数计算分裂点。
   2. 系统设计
      1. 核外块运算：
         1. 块拆分。将不能存储在内存的数据放在磁盘中。并且拆分成块，以提高磁盘IO率。
         2. 块压缩。每个块存储时会压缩存储。读取时解压读取。
         3. 开启线程在运算数据的同时对数据进行读取。
      2. 分块并行。
         1. 预先对所有的特征进行按列预排序，并存储索引。这样每次分裂只需要一次线性扫描就能获得最大的Gain
         2. 分布式运行：将所有的特征分成N各块，发送给各个机器执行。最后将执行结果返回给调度中心。
      3. 缓存命中率优化。由于特征存储为block结构。所以访问不是顺序访问的。为此在block后面开一个buffer，存储他们的一阶导和二阶导。直接读取一阶导和二阶导。这样命中率就会提高。
3. 对缺失值的处理：
   1. 所有的计算对非缺失值进行计算。计算完成后，缺失值整体放入左右，计算增益。设置最大增益方向为缺省（默认）方向放入缺失值。
   2. 预测中遇到缺失值，默认放入右节点。


# 题目

参考：https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485159&idx=1&sn=d429aac8370ca5127e1e786995d4e8ec&chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&scene=21#wechat_redirect

https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/XGBoost.md


1. XGBoost特征重要性。


2. 介绍一下XGB，怎么调参防止过拟合