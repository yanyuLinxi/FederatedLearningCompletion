
# 极大似然估计

1. 概率
   1. 在知道**分布规律**和**具体参数**的情况下，可以计算出某个事件发生的概率。
   2. 举例：抛硬币10次，5次正面朝上的概率。分布规律为二项分布。参数为正面朝上的概率为1/2。随意概率为$c^5_{10}*0.5^5*(1-0.5)^5=0.25$
2. 估计
   1. 不知道分布规律和具体参数，进行猜测。即为估计。
3. 极大似然估计
   1. 似然函数（联合概率）公式：将多个概率值相乘。$L=\prod_{i=1}^n p\{x=k_i|p\}$，k_i为事件。P_k为概率。
   2. 优化目标：$\hat{p}=argmax_p L$. 求一个参数使得出现现有样本可能性最大。
   3. 方法：取对数 $lnL$然后拟牛顿法，或者梯度下降。
4. 步骤：
   1. 确定分布/模型
   2. 进行多组实验并观察客观现象
   3. 写出似然概率（联合概率）
   4. 求解似然函数最大的凸优化问题，求的参数（梯度下降）

# 梯度下降

1. 原理：将参数沿着梯度相反的方向前进一个步长，就可以实现目标函数（loss function）的下降。
2. 方法：
   1. 使用损失函数，对每个参数（变量）求偏导数
   2. 将值带入，求下降方向$J(\theta)$
   3. 根据步长$\alpha$，进行一步梯度下降 $\theta <= \theta-\alpha J(\theta)$ 

# 交叉熵损失
1. 公式$f(w)=-\frac{1}{m}\sum_{i=1}^N(y^ilogf_w(x^i)+(1-y^i)log(1-x^i))$
2. 推导：
   1. 对于每一个样本，设它为1的概率为p，则为0的概率为1-p
   2. 即
   $$
      P(y|x)=
         \begin{cases}
         p& \text{y=1}\\
         1-p& \text{y=0}
         \end{cases}
   $$
   3. 上述式子不方便计算。其等价于$p(y_i|x_i)=p^{y_i}(1-p)^{1-y_i}$。 即当$y_i=1$时，该值为p,否则为1-p
   4. 其最大似然估计为$L=\prod_{i=1}^n p^{y_n}(1-p)^{1-y_n}$。在逻辑回归中，这里面只有一个参数就在p里面。
   5. 上个式子不好计算，所以我们取对数$F ( w ) = \ln ( P _ { 总} ) \\= \ln ( \prod _ { n = 1 } ^ { N } p ^ { y _ { n } }( 1 - p ) ^ { 1 - y _ { n } } ) \\= \sum _ { n = 1 } ^ { N } ( y _ { n } \ln ( p ) + ( 1 - y _ { n } ) \ln ( 1 - p ) )$。 最终为求使得这个值最大的w。添加一个符号即为求最小值。即可以用梯度下降。



# 公式推导

1. 背景知识
   1. 1. **回归**：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，**无限次的进行测量**，最后通过这些测量数据计算**回归到真实值**，这就是回归的由来。通过**有监督的学习**，学习到由x到y的映射h，利用该映射关系对未知的数据进行预估，因为y为连续值，所以是**回归问题。**
   2. **线性回归的损失函数**。一般使用最小二乘法，则MSE误差平方为$(yi−hθ(xi))^2$找到合适的参数
   3. 逻辑回归主要用来解决**分类问题**，线性回归的结果Y带入一个非线性变换的Sigmoid函数中，得到[0,1]之间取值范围的数S，**S可以把它看成是一个概率值**，如果我们设置概率阈值为0.5，那么S大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。
2. 逻辑回归
   1. 逻辑回归=线性回归+sigmoid函数
   2. 公式: $z=wx+b, y=\frac{1}{1+e^{-z}}$。判别式是$f(x)=sign(y)$
   3. 损失函数（交叉熵损失函数）:$L = - \sum_i^n (ylogp+(1-y)log(1-p))$。 前面的y是标签，后面的p是预测出来的分布。
      1. 损失函数求导。损失函数:$L=- \sum _ { n = 1 } ^ { N } ( y _ { n } \ln ( p ) + ( 1 - y _ { n } ) \ln ( 1 - p ) )$,这里的$p=\frac{1}{1+e^{-z}}$就是这个值的概率。
      2. $\frac{\partial f(x)}{\partial x}=\frac{1}{1+e^{-z}}*\frac{e^{-z}}{1+e^{-z}}=p(1-p)$
      3. 最终求导结果为$\frac{\partial L}{\partial x}= -\frac{1}{n}\sum_{n=1}^{N}(y_i-\frac{1}{1+e^{-z}})$
3. 案例：
   1. p(Y=1|x)=p(x)
   2. p(y=0|x)=1-p(x).p就是函数
4. 极大似然估计

推导W、b的梯度下降步骤。

# 面试相关

1. 如何多分类：
   1. 一对一：N个类别两两配对
   2. 一对多：每次将一个例作为正例。其他作为反例。
   3. 多对多。多个类正类，多个类反类。需要进一步的设置。
2. 逻辑回归优缺点：
   1. 优点
      1. LR的可解释性强、可控度高、训练速度快
   2. 缺点：
      1. 特征工程复杂。需要较多的特征工程和归一化。

3. **逻辑斯特回归为什么要对特征进行离散化**  （ 对特征离散化，让特征里没有连续值）
   1. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合
   2. 稀疏向量内积乘法运算速度快计算结果方便存储，容易扩展
   3. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。
4. **线性回归**与逻辑回归的区别
   1. 线性回归主要解决回归任务，逻辑回归主要解决分类问题。
   2. 线性回归的输出一半是连续的，逻辑回归的输出一般是离散的。
   3. 线性回归的损失函数是MSE,逻辑回归中，采用的是负对数损失函数
5. **为什么逻辑回归比线性回归要好**？
   1.  在特征到结果的映射中加入了一层sigmoid函数（非线性）映射，即先把特征线性求和，
   2.  另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在0,1间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。
6.  **LR为什么使用sigmoid函数？**
    1.  而Sigmoid能够把它映射到[0,1]之间。正好这个是概率的范围。
    2.  Sigmoid是连续光滑的。
    3.  Sigmoid也让逻辑回归的损失函数成为凸函数，这也是很好的性质。可以寻找到一个全局最优点进行下降。
7.  **LR和SVM有什么不同吗**
    1.  相同
        1.  两个方法都可以增加不同的正则化项
        2.  LR和SVM都可以**处理分类问题，且一般都用于处理线性二分类问题**
    2.  不同：
        1.  LR是参数模型，SVM是非参数模型。参数模型即需要知道数据的分布参数。
        2.  **从目标函数来看**，区别在于逻辑回归采用的是交叉熵损失函数，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
        3.  SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
        4.  **SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,**这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
        5.  LR能做的 SVM能做，但可能在准确率上有问题，SVM能做的LR有的做不了。
    3.  总结。模型类型，从损失函数，支持向量上面进行分析。
8. sigmoid作为激活函数的缺点（难算，非中心化，梯度消失）